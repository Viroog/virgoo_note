# token相关

- 加载tokenizer，一般使用AutoTokenizer即可

  ```python
  tokenizer=AutoTokenizer.from_pretrained()
  ```

- 加入新token。在生成式推荐中，一个物品由D个token来表示。将物品的每一个token当作词表的新token加入，重头开始训练。只是在预训练好的参数的基础上继续训练，可以理解为**为微调找到了一个比较好的起点**

  ```python
  # 返回新加入的token数量
  add_num = tokenizer.add_tokens(train_dataset.get_new_tokens())
  ```

- 加载并更新config

  ```python
  config = AutoConfig.from_pretrained(model_path)
  config.vocab_size = len(tokenizer)
  ```

- 保存新的tokenizer和config

  ```python
  # local_rank==0表示当前进程为主进程(避免重复保存)
  if locak_rank==0:
      tokenizer.save_pretrained(args.output_dir)
  	config.save_pretrained(args.output_dir)
  ```



# Lora相关

- 参数相关

  | 参数名称        | 作用                                                         | 其他相关                                                     |
  | --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | task_type       | 根据模型的类型来指定该参数                                   |                                                              |
  | r               | 两个低秩矩阵A和B的秩                                         |                                                              |
  | target_modules  | 指定在LLM中的哪些模块中加入Lora。如果取默认值则会根据不同的大模型架构决定，比如T5只会对q,v加Lora | 类型可以是单个字符串(多个模块用逗号隔开)，也可是字符串列表，还可以使用正则表达式(方便对一整模块进行操作) |
  | lora_alpha      | 低秩矩阵的缩放因子，实际上的取值为lora_alpha/r               |                                                              |
  | modules_to_save | 除了lora要进行训练，模型还有哪些部分需要被训练               |                                                              |

- 配置Lora并获得相对应的模型

  ```python
  lora_config = LoraConfig()
  
  model=get_peft_model(model, lora_config)
  ```

- 训练完成后，将lora和原本模型进行合并。**不然需要额外的计算代价？可能是需要多做一次乘法(输入与lora相乘)和加法(两条支路的结果进行合并)**

  ```python
  #1 导包
  from peft import PeftModel
  
  #2 加载原始模型(原始模型是指下载得到的模型)
  model=AutoModel.from_pretrained()
  
  #3 加载lora adapter, 其中model_id为checkpoint的文件夹用于加载lora
  model=PeftModel.from_pretrained(model, model_id='')
  
  # 保存合并模型
  merged_model=model.merge_and_unload()
  ```



# Dataset相关

- Dataset可以使用Huggingface自带的dataset接口，也可以使用pytorch中的dataset库。前者在纯NLP任务中使用的更多，后者适用于任何数据。

- 在重写`__getitem__`方法时，返回类型是字典，并且输入序列数据的key**必须是input_ids**，标签的key**必须是labels**。主要原因是为了collator函数的识别

  ```python
  def __getitem__(self, index):
  
  	data = self.data[index]
  
  	return {
  		'input_ids': data['history'],
  		'labels': data['target']
  	}
  ```



# Model相关

## 不重构模型

- 这里只记录与本身科研相关的一些AutoModel类。适用任务的参考价值并不大，因为现在的CausalLM已经强大到能够完成大部分任务了(专才和通才的关系)

  | 类名                  | 描述                                                         | 适用任务                         |
  | --------------------- | ------------------------------------------------------------ | -------------------------------- |
  | AutoModel             | 加载预训练模型的基础模型(相当于父类)，不包含任何任务特定的头部 | 特征提取、嵌入生成、自定义任务等 |
  | AutoModelForCausalLM  | 加载带有因果语言建模头部的模型，适用于生成任务，如LLaMa、GPT | 文本生成、对话系统、自动补全等   |
  | AutoModelForSeq2SeqLM | 加载适用于序列到序列任务的模型，带有Encoder-decoder架构      | 机器翻译、文本摘要、问答系统等   |

- 如果在tokenizer阶段加入了新的token，要在这里对新token的embedding进行**初始化**

  ```1python
  model.resize_token_embeddings(len(tokenizer))
  ```

- 



## 重构模型

- 在自己的模型中继承huggingface中的对应的类，并重写自己需要的函数

  ```python
  class M3Rec(T5ForConditionalGeneration):
      # rewrite
      def __init__(self, config, args):
          super(M3Rec, self).__init__(config)
          
          # 完成初始化逻辑
  
      @classmethod
      # rewrite
      # cls: M3Rec
      def from_pretrained(
              cls,
              pretrained_model_name_or_path,
              *model_args,
              config=None,
              cache_dir=None,
              ignore_mismatched_sizes=False,
              force_download=False,
              local_files_only=False,
              token=None,
              revision="main",
              use_safetensors=None,
              **kwargs,
      ) -> "PreTrainedModel":
          
          # 重写加载预训练权重的方法，需要注意的是需要添加@classmethod，对cls这个参数进行识别
          # cls(config, *model_args, **kwargs)与调用self.__init__()一致
  
      def forward(
              self,
              input_ids=None,
              item_position_ids=None,
              meta_type_ids=None,
              attention_mask=None,
              decoder_input_ids=None,
              decoder_attention_mask=None,
              head_mask=None,
              decoder_head_mask=None,
              cross_attn_head_mask=None,
              encoder_outputs=None,
              past_key_values=None,
              inputs_embeds=None,
              decoder_inputs_embeds=None,
              labels=None,
              use_cache=None,
              output_attentions=None,
              output_hidden_states=None,
              return_dict=None,
      ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:
  
        # 你需要进行的各种处理逻辑，比如设计不同的loss，设计更复杂的输入  
        
  ```

  



# TrainingArguments相关

- 与Model部分一样，

  | 参数名称                 | 作用                                                         | 其他相关                                                     |
  | ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | max_new_tokens           | 控制要生成的token数量                                        | 默认为0                                                      |
  | num_beams                | beam_search的数目                                            | 相关资料：https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html |
  | num_return_sequence      | 指定最终返回生成序列的数量，要求小于等于num_beams            |                                                              |
  | output_scores            | 指定是否返回生成的token sequence的分数，在生成式推荐中将这个分数简单理解为用户对物品的偏好程度 | Bool类型，默认为False                                        |
  | return_dict_in_generate  | 控制返回结果是字典还是元组，True时返回字典                   | Bool类型，默认为False                                        |
  | early_stopping           | True：只有在生成num_beams个候选项时才停止搜索<br />False：应用[启发式方法](https://zhida.zhihu.com/search?content_id=233445660&content_type=Article&match_order=1&q=启发式方法&zhida_source=entity)，当很不可能找到更好的候选项时停止生成<br/>never：只有当不能找到更好的候选项时，束搜索过程才会停止 | Bool类型                                                     |
  | prefix_allowed_tokens_fn | **目前的理解是使用字典树来约束大模型的生成**                 | Callback函数                                                 |



# 训练

- 创建相对应的Trainer，并传递必要的参数

  ```python
  # 以Seq2Seq Model为例
  trainer = Seq2SeqTrainer(model=model,
                           train_dataset=train_dataset,
                           eval_dataset=train_dataset,
                           args=training_args,
                           tokenizer=tokenizer,
                           data_collator=collator,
                           callbacks=[EarlyStoppingCallback(early_stopping_patience=10)])
  ```

  

- 直接调用trainer.train()即可



# 推理

- 推理阶段使用generate函数，除了接受必要的输入inputs_ids和attention_mask外，还有以下重要的参数

  - 相关参考链接：

    

  | 参数名称                 | 作用                                                         | 其他相关                                                     |
  | ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | max_new_tokens           | 控制要生成的token数量                                        | 默认为0                                                      |
  | num_beams                | beam_search的数目                                            | 相关资料：https://zh.d2l.ai/chapter_recurrent-modern/beam-search.html |
  | num_return_sequence      | 指定最终返回生成序列的数量，要求小于等于num_beams            |                                                              |
  | output_scores            | 指定是否返回生成的token sequence的分数，在生成式推荐中将这个分数简单理解为用户对物品的偏好程度 | Bool类型，默认为False                                        |
  | return_dict_in_generate  | 控制返回结果是字典还是元组，True时返回字典                   | Bool类型，默认为False                                        |
  | early_stopping           | True：只有在生成num_beams个候选项时才停止搜索<br />False：应用[启发式方法](https://zhida.zhihu.com/search?content_id=233445660&content_type=Article&match_order=1&q=启发式方法&zhida_source=entity)，当很不可能找到更好的候选项时停止生成<br/>never：只有当不能找到更好的候选项时，束搜索过程才会停止 | Bool类型                                                     |
  | prefix_allowed_tokens_fn | **目前的理解是使用字典树来约束大模型的生成**                 | Callback函数                                                 |



# 一些细节

- 在开始训练之前，将use_cache设置为False。use_cache用于推理阶段，决定是否用之前计算过的key和value去加速LLM推理速度。

  相关参考资料：https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn

  ​			   https://zhuanlan.zhihu.com/p/667471165

- 在多卡训练的情况下，使用local_rank来获取主进程，主进程的id为0。可以用于进行那些只需要执行一次的操作。

  ```python
  #获取local_rank的方式
  local_rank=int(os.environ.get('LOCAL_RANK') or 0)
  ```

- 